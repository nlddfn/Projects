mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',as.character(Country)))
#
View(dataOECD)
View(dataOECD)
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',as.character(Country))
View(dataOECD)
View(dataOECD)
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',Country)))
View(dataOECD)
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',make.names(Country))))
View(dataOECD)
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',identity(Country))))
View(dataOECD)
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',Country[Country])))
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',paste(Country)))
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',paste(Country)))
dataOECD = read.csv('Project 1/BLI_10072016_Clean.csv',sep = ";")
dataOECD = tbl_df(dataOECD)%>%
mutate(.,Indicator = make.names(Indicator),
Country = ifelse(Country=='United States','USA',
ifelse(Country=='United Kingdom', 'UK',paste(Country))))
View(dataOECD)
is.positiveIndicator = function(x){
impact = data.frame(Indicator = unique(dataOECD$Indicator),
Value = c(F,F,T,T,T,T,F,T,T,T,T,T,F,T,T,T,T,T,F,F,T,F,T,T)) %>%
spread(.,Indicator,Value)
return(impact$x)
}
dataOECD_normalize = dataOECD %>% group_by(.,Indicator,Inequality)%>%
mutate(.,Value = ifelse(is.positiveIndicator(Indicator),
100*(Value - min(Value))/(max(Value) - min(Value)),
100*(1-(Value - min(Value))/(max(Value) - min(Value)))
))
View(dataOECD_normalize)
is.positiveIndicator(Life.satisfaction)
impact = data.frame(Indicator = unique(dataOECD$Indicator),
Value = c(F,F,T,T,T,T,F,T,T,T,T,T,F,T,T,T,T,T,F,F,T,F,T,T)) %>%
spread(.,Indicator,Value)
impact
impact$Air.pollution
impact$Water.quality
is.positiveIndicator = function(x){
impact = data.frame(Indicator = unique(dataOECD$Indicator),
Value = c(F,F,T,T,T,T,F,T,T,T,T,T,F,T,T,T,T,T,F,F,T,F,T,T)) %>%
spread(.,Indicator,Value)
return(impact$as.factor(x))
}
is.positiveIndicator('Life.satisfaction')
impact = data.frame(Indicator = unique(dataOECD$Indicator),
Value = c(F,F,T,T,T,T,F,T,T,T,T,T,F,T,T,T,T,T,F,F,T,F,T,T)) %>%
spread(.,Indicator,Value)
impact
impact[,'Life.expectancy']
is.positiveIndicator = function(x){
impact = data.frame(Indicator = unique(dataOECD$Indicator),
Value = c(F,F,T,T,T,T,F,T,T,T,T,T,F,T,T,T,T,T,F,F,T,F,T,T)) %>%
spread(.,Indicator,Value)
return(impact[,x])
}
is.positiveIndicator('Life.expectancy')
dataOECD_normalize = dataOECD %>% group_by(.,Indicator,Inequality)%>%
mutate(.,Value = ifelse(is.positiveIndicator(Indicator),
100*(Value - min(Value))/(max(Value) - min(Value)),
100*(1-(Value - min(Value))/(max(Value) - min(Value)))
))
View(dataOECD_normalize)
dataOECD_aggregate = dataOECD_normalize  %>%
select(.,-4) %>%
filter(., Inequality == "Total" & Country !='OECD - Total' )%>%
spread(.,Indicator,Value) %>%
transmute(., region = Country,
Housing = (Rooms.per.person + Housing.expenditure + Dwellings.with.basic.facilities)/3,
Wealth = (Household.net.adjusted.disposable.income + Household.net.financial.wealth)/2,
Jobs = (Job.security + Personal.earnings + Long-term.unemployment.rate+Employment.rate)/4,
Quality.of.support.network = Quality.of.support.network,
Life.satisfaction = Life.satisfaction,
Education = (Years.in.education + Student.skills + Educational.attainment)/3,
Environment = (Water.quality + Air.pollution)/2,
Engagement = (Stakeholder.engagement.for.developing.regulations + Voter.turnout)/2,
Health = (Self.reported.health + Life.expectancy)/2,
Safety = (Homicide.rate + Feeling.safe.walking.alone.at.night)/2,
WorkLife = (Time.devoted.to.leisure.and.personal.care + Employees.working.very.long.hours)/2
)
dataOECD_aggregate = dataOECD_normalize  %>%
select(.,-4) %>%
filter(., Inequality == "Total" & Country !='OECD - Total' )%>%
spread(.,Indicator,Value) %>%
transmute(., region = Country,
Housing = (Rooms.per.person + Housing.expenditure + Dwellings.without.basic.facilities)/3,
Wealth = (Household.net.adjusted.disposable.income + Household.net.financial.wealth)/2,
Jobs = (Job.security + Personal.earnings + Long-term.unemployment.rate+Employment.rate)/4,
Quality.of.support.network = Quality.of.support.network,
Life.satisfaction = Life.satisfaction,
Education = (Years.in.education + Student.skills + Educational.attainment)/3,
Environment = (Water.quality + Air.pollution)/2,
Engagement = (Stakeholder.engagement.for.developing.regulations + Voter.turnout)/2,
Health = (Self.reported.health + Life.expectancy)/2,
Safety = (Homicide.rate + Feeling.safe.walking.alone.at.night)/2,
WorkLife = (Time.devoted.to.leisure.and.personal.care + Employees.working.very.long.hours)/2
)
View(dataOECD)
dataOECD_aggregate = dataOECD_normalize  %>%
select(.,-4) %>%
filter(., Inequality == "Total" & Country !='OECD - Total' )%>%
spread(.,Indicator,Value) %>%
transmute(., region = Country,
Housing = (Rooms.per.person + Housing.expenditure + Dwellings.without.basic.facilities)/3,
Wealth = (Household.net.adjusted.disposable.income + Household.net.financial.wealth)/2,
Jobs = (Labour.market.insecurity + Personal.earnings + Long-term.unemployment.rate+Employment.rate)/4,
Quality.of.support.network = Quality.of.support.network,
Life.satisfaction = Life.satisfaction,
Education = (Years.in.education + Student.skills + Educational.attainment)/3,
Environment = (Water.quality + Air.pollution)/2,
Engagement = (Stakeholder.engagement.for.developing.regulations + Voter.turnout)/2,
Health = (Self.reported.health + Life.expectancy)/2,
Safety = (Homicide.rate + Feeling.safe.walking.alone.at.night)/2,
WorkLife = (Time.devoted.to.leisure.and.personal.care + Employees.working.very.long.hours)/2
)
names(dataOECD)
dataOECD$Indicator
dataOECD_aggregate = dataOECD_normalize  %>%
select(.,-4) %>%
filter(., Inequality == "Total" & Country !='OECD - Total' )%>%
spread(.,Indicator,Value) %>%
transmute(., region = Country,
Housing = (Rooms.per.person + Housing.expenditure + Dwellings.without.basic.facilities)/3,
Wealth = (Household.net.adjusted.disposable.income + Household.net.financial.wealth)/2,
Jobs = (Labour.market.insecurity + Personal.earnings + Long.term.unemployment.rate+Employment.rate)/4,
Quality.of.support.network = Quality.of.support.network,
Life.satisfaction = Life.satisfaction,
Education = (Years.in.education + Student.skills + Educational.attainment)/3,
Environment = (Water.quality + Air.pollution)/2,
Engagement = (Stakeholder.engagement.for.developing.regulations + Voter.turnout)/2,
Health = (Self.reported.health + Life.expectancy)/2,
Safety = (Homicide.rate + Feeling.safe.walking.alone.at.night)/2,
WorkLife = (Time.devoted.to.leisure.and.personal.care + Employees.working.very.long.hours)/2
)
View(dataOECD_aggregate)
# 2-Prepare world map
map.world = map_data(map="world")
base = ggplot(data = map.world, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(fill = NA, color = "gray10")+
coord_fixed(xlim = c(-180, 180.0),  ylim = c(-55, 90), ratio = 1.3) +
xlab("Longitude") + ylab("Latitude")
map_ind_join = inner_join(map.world, dataOECD_aggregate, by = "region")
base + geom_polygon(data = map_ind_join, aes(fill = Health,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 5, 10, 15, 20, 25, 30))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Environment,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 5, 10, 15, 20, 25, 30))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Safety,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 10, 20, 30, 40, 50, 60,70,80,90,100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, show.legend = F
aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, show.legend = F
aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradientn(name = "Legend", low = "#132B43", high = "#56B1F7",
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, show.legend = F
aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradientn(name = "Legend", low = "#132B43", high = "#56B1F7")+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, show.legend = F
aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradient(name = "Legend", low = "#132B43", high = "#56B1F7")+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Life.satisfaction,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Housing,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Income,alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
base + geom_polygon(data = map_ind_join, aes(fill = Wealth, alpha=.5)) +
scale_fill_gradientn(name = "Legend", colours = rev(rainbow(7)),
breaks = c(0, 20, 40, 60, 80, 100))+
theme(legend.position = "bottom")
install.packages("googleVis")
install.packages("shinydashboard")
library(plotly)
shiny::runApp('Project 2')
runApp('Project 2')
library(xts)
runApp('Project 2')
runApp('Project 2')
runApp('Project 2')
runApp('Project 2')
runApp('Project 2')
L = leaflet(balkanRoute.map) %>%
addProviderTiles("CartoDB.Positron") %>%
setView(lat = 42, lng = 20, zoom = 5)
library(dplyr)
library(leaflet)
L = leaflet(balkanRoute.map) %>%
addProviderTiles("CartoDB.Positron") %>%
setView(lat = 42, lng = 20, zoom = 5)
L = leaflet() %>%
addProviderTiles("CartoDB.Positron") %>%
setView(lat = 42, lng = 20, zoom = 5)
L
shiny::runApp('Project 2')
shiny::runApp('Project 2')
runApp('Project 2')
shiny::runApp('Project 3')
runApp('Project 3')
runApp('Project 3/Published/GlassShiny')
install.packages(c("DBI", "DT", "R6", "Rcpp", "car", "curl", "digest", "dygraphs", "htmlwidgets", "knitr", "maps", "mgcv", "stringr", "tidyr"))
shiny::runApp('Project 3/Published/GlassShiny')
library(caret)
library(caretEnsemble)
library(xgboost)
library(Ckmeans.1d.dp)
library(tidyr)
library(ggplot2)
library(doMC)
library(Hmisc) #missingness imputation
library(slice)
library(ada)
library(randomForest)
install.packages(c("irlba", "tibble"))
library(caret)
library(caretEnsemble)
library(xgboost)
library(Ckmeans.1d.dp)
library(tidyr)
library(ggplot2)
library(doMC)
library(Hmisc) #missingness imputation
library(slice)
library(ada)
library(randomForest)
library(caret)
library(caret)
library(lattice)
library(ggplot2)
library(caretEnsemble)
library(xgboost)
library(Ckmeans.1d.dp)
library(tidyr)
library(ggplot2)
library(tidyr)
library(doMC)
library(foreach)
library(plyr)
library(dplyr)
library(lattice)
library(ggplot2)
library(caret)
library(caretEnsemble)
library(xgboost)
library(Ckmeans.1d.dp)
library(tidyr)
library(iterators)
library(parallel)
library(foreach)
library(doMC)
library(Hmisc) #missingness imputation
library(slice)
library(ada)
library(rpart)
library(ada)
library(randomForest)
library("xgboost", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
data(agaricus.train, package='xgboost')
# Both dataset are list with two items, a sparse matrix and labels
# (labels = outcome column which will be learned).
# Each column of the sparse Matrix is a feature in one hot encoding format.
train <- agaricus.train
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
# train$data@Dimnames[[2]] represents the column names of the sparse matrix.
xgb.importance(train$data@Dimnames[[2]], model = bst)
# Same thing with co-occurence computation this time
xgb.importance(train$data@Dimnames[[2]], model = bst, data = train$data, label = train$label)
gb.plot.importance(importance_matrix)
xgb.plot.importance(importance_matrix)
data(agaricus.train, package='xgboost')
#Both dataset are list with two items, a sparse matrix and labels
#(labels = outcome column which will be learned).
#Each column of the sparse Matrix is a feature in one hot encoding format.
train <- agaricus.train
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
#train$data@Dimnames[[2]] represents the column names of the sparse matrix.
importance_matrix <- xgb.importance(train$data@Dimnames[[2]], model = bst)
xgb.plot.importance(importance_matrix)
2-We can identify which airports, terminals and companies are more likely to offer an access to Spotify as a paid service or compensation for any inconvenience, using the information on flight delays...
shiny::runApp('Project 3/Published/GlassShiny')
tmp
tmp = data.frame(pred = 1,obs=2)
grp_cols <-names(tmp)
Joey(grp_cols,tmp)
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by_(data, dots) %>%
summarise(total= sum(TRXN_AMOUNT), count = n()) %>%
mutate(avg = total/count)
return( out )
}
Joey(grp_cols,tmp)
library(dplyr)
Joey(grp_cols,tmp)
dots <- lapply(grp_cols, as.symbol)
dots
grp_cols <-names(tmp)[1]
Joey(grp_cols,tmp)
f <-  data.frame(
asihckhdoydk = sample(LETTERS[1:3], 100, replace=TRUE),
a30mvxigxkgh = sample(LETTERS[1:3], 100, replace=TRUE),
value = rnorm(100)
)
# Columns you want to group by
grp_cols <- names(df)[-3]
# Convert character vector to list of symbols
dots <- lapply(grp_cols, as.symbol)
# Perform frequency counts
df %>%
group_by_(.dots=dots) %>%
summarise(n = n())
df <-  data.frame(
asihckhdoydk = sample(LETTERS[1:3], 100, replace=TRUE),
a30mvxigxkgh = sample(LETTERS[1:3], 100, replace=TRUE),
value = rnorm(100)
)
# Columns you want to group by
grp_cols <- names(df)[-3]
# Convert character vector to list of symbols
dots <- lapply(grp_cols, as.symbol)
# Perform frequency counts
df %>%
group_by_(.dots=dots) %>%
summarise(n = n())
grp_cols = names(tmp)
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by(data, dots)
return( out )
}
penguin = Joey(grp_cols, tmp)
grp_cols = names(tmp)[1]
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by(data, dots)
return( out )
}
penguin = Joey(grp_cols, tmp)
grp_cols = names(tmp)[1]
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by(data, grp_cols)
return( out )
}
penguin = Joey(grp_cols, tmp)
grp_cols = names(tmp)[1]
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by(data, grp_cols)
return( out )
}
penguin = Joey(grp_cols, tmp)
tmp$pred
# Calculate grouping function
grp_cols = names(tmp)[1]
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by(data, .dots = grp_cols)
return( out )
}
penguin = Joey(grp_cols, tmp)
grp_cols = names(tmp)[1]
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by_(data, .dots = dots)
return( out )
}
penguin = Joey(grp_cols, tmp)
Joey = function(grp_cols, data){
dots <- lapply(grp_cols, as.symbol)
out <- group_by_(data, .dots = dots)
return( out )
}
grp_cols = names(tmp)[1]
penguin = Joey(grp_cols, tmp)
penguin
# Data Manipulation and visualization
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(corrplot)
# Missingness and imputation
library(Hmisc)
library(VIM)
library(mice)
# Modeling
library(caret)
library(xgboost)
# Load functions and data dir
set.seed(0)
setwd("~/Documents/Git_stuff/Projects/BIdatachallenge")
source("~/Documents/R code/R_fun_collection.R")
##### FUNCTIONS #####
impute_star <- function(obs,df){
tmp = filter(df, df$city_id == obs) %>%
group_by(., stars) %>%
summarise(.,count = n()) %>%
filter(., count == max(count))
if(length(tmp$stars == 1) && is.na(tmp$stars)) return(0)
if(length(tmp$stars)>1) return(max(tmp$stars))
else return(tmp$stars)
}
####### The wMSE function defined according to Trivago
wMSE = function(real,pred){
# weight
w = log(real + 1) + 1
error = t(as.matrix(w)) %*% as.matrix((pred - real)^2)/ sum(w)
return(error)}
wMSE_summary <- function(data, lev = NULL, model = NULL){
out = (wMSE(data$obs, data$pred))
names(out) <- "wMSE"
return(out)
}
evalerror <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- wMSE(labels, preds)
return(list(metric = "error", value = err))
}
#load dataset with size [832085, 10]
df <- read.csv("data_recruiting_bi_data_challenge.csv", header=T, row.names = 1)
colnames(df) = c('hotel_id',  'city_id', 'clicks', 'stars', 'distance',
'avg_price', 'rating','num_part', 'rel_saving', 'avg_rank')
# Show basic EDA
open_sesame(select(df,-3))
# Drop negative values
df[df == -1] <- NA
# set subsample data for visualization: 1% of the complete cases. Drop hotel_id,
# city_id and clicks.
good.cases = complete.cases(df)
df.good.cases = df[good.cases,c(4:10)]
df.subsample <- df.good.cases[sample(1:nrow(df.good.cases),
nrow(df.good.cases)/100, replace=FALSE),]
# Calculate frequency table for city_id. Add as extra feat,remove categoricals
df_hotel_id = df['hotel_id']
freq_tabl = table(df$city_id)
df = df%>% mutate(.,city_freq=freq_tabl[as.character(city_id)]) %>% select(.,-hotel_id,-city_id)
# Split training and target dataset.
df_target     <- filter(df,is.na(df$clicks))
df_train_all  <- filter(df,is.na(df$clicks)== FALSE)
# Partition train in train_cv and test using Data Partion in R caret
# (stratification is unnecessary because the label is an integer)
train.index <- createDataPartition(df_train_all$clicks, p = .8, list = FALSE)
df_train    <- df_train_all[ train.index,]
df_test     <- df_train_all[-train.index,]
# create sparse matrices for XGB.
df_train_sparse  <- xgb.DMatrix(data = data.matrix(df_train[,-1]) , label = df_train$clicks,  missing = NA)
df_test_sparse   <- xgb.DMatrix(data = data.matrix(df_test[,-1])  , label = df_test$clicks,   missing = NA)
df_target_sparse <- xgb.DMatrix(data = data.matrix(df_target[,-1]), label = df_target$clicks, missing = NA)
View(df)
freq_tabl
freq_tabl[1]
freq_tabl[2]
freq_tabl[city_id[1]]
freq_tabl[city_id]
df = df%>% mutate(.,city_freq=freq_tabl[city_id]) %>% select(.,-hotel_id,-city_id)
